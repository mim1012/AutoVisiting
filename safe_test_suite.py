#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G4K ìë™í™” ì‹œìŠ¤í…œ ì•ˆì „ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
ì˜ì¡´ì„± ë¬¸ì œë¥¼ ìš°íšŒí•˜ëŠ” ë…ë¦½ì ì¸ í…ŒìŠ¤íŠ¸
"""

import unittest
import json
import os
import tempfile
import time
import logging
from datetime import datetime
from typing import Dict, List, Any
import sys

# ì•ˆì „í•œ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SafeTestResult:
    """ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
    
    def __init__(self):
        self.results = []
        self.summary = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'skipped': 0,
            'errors': []
        }
    
    def add_test_result(self, test_name: str, status: str, message: str = "", execution_time: float = 0):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        result = {
            'test_name': test_name,
            'status': status,
            'message': message,
            'execution_time': execution_time,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results.append(result)
        self.summary['total'] += 1
        
        if status == 'PASSED':
            self.summary['passed'] += 1
        elif status == 'FAILED':
            self.summary['failed'] += 1
            self.summary['errors'].append(f"{test_name}: {message}")
        elif status == 'SKIPPED':
            self.summary['skipped'] += 1
    
    def get_summary(self):
        """ìš”ì•½ ë°˜í™˜"""
        success_rate = (self.summary['passed'] / self.summary['total'] * 100) if self.summary['total'] > 0 else 0
        return {
            **self.summary,
            'success_rate': round(success_rate, 2)
        }


class ConfigurationTests:
    """ì„¤ì • ë° êµ¬ì„± í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.result = SafeTestResult()
    
    def test_required_files_exist(self):
        """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        start_time = time.time()
        test_name = "required_files_exist"
        
        try:
            required_files = [
                'requirements.txt',
                'config.yaml',
                'CLAUDE.md',
                'README.md'
            ]
            
            missing_files = []
            existing_files = []
            
            for file in required_files:
                if os.path.exists(file):
                    existing_files.append(file)
                else:
                    missing_files.append(file)
            
            if missing_files:
                self.result.add_test_result(
                    test_name,
                    'FAILED',
                    f"Missing files: {', '.join(missing_files)}",
                    time.time() - start_time
                )
            else:
                self.result.add_test_result(
                    test_name,
                    'PASSED',
                    f"All {len(existing_files)} required files found",
                    time.time() - start_time
                )
                
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_python_modules_exist(self):
        """íŒŒì´ì¬ ëª¨ë“ˆ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        start_time = time.time()
        test_name = "python_modules_exist"
        
        try:
            required_modules = [
                'stealth_browser.py',
                'profile_manager.py',
                'adaptive_calendar_refresher.py',
                'ultra_lag_bypass.py',
                'multi_profile_ticketing.py',
                'cancellation_hunter.py'
            ]
            
            existing_modules = []
            missing_modules = []
            
            for module in required_modules:
                if os.path.exists(module):
                    existing_modules.append(module)
                else:
                    missing_modules.append(module)
            
            self.result.add_test_result(
                test_name,
                'PASSED' if not missing_modules else 'FAILED',
                f"Found {len(existing_modules)}/{len(required_modules)} modules. Missing: {missing_modules}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_json_files_valid(self):
        """JSON íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        start_time = time.time()
        test_name = "json_files_valid"
        
        try:
            json_files = [
                'user_profiles.json',
                'auto_check_settings.json',
                'reservation_templates.json'
            ]
            
            valid_files = []
            invalid_files = []
            
            for json_file in json_files:
                if os.path.exists(json_file):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            json.load(f)
                        valid_files.append(json_file)
                    except json.JSONDecodeError as e:
                        invalid_files.append(f"{json_file}: {str(e)}")
                else:
                    invalid_files.append(f"{json_file}: File not found")
            
            self.result.add_test_result(
                test_name,
                'PASSED' if not invalid_files else 'FAILED',
                f"Valid: {len(valid_files)}, Invalid: {len(invalid_files)}. Details: {invalid_files[:3]}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_requirements_parseable(self):
        """requirements.txt íŒŒì‹± ê°€ëŠ¥ ì—¬ë¶€"""
        start_time = time.time()
        test_name = "requirements_parseable"
        
        try:
            if not os.path.exists('requirements.txt'):
                self.result.add_test_result(
                    test_name,
                    'SKIPPED',
                    "requirements.txt not found",
                    time.time() - start_time
                )
                return
            
            with open('requirements.txt', 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            packages = []
            invalid_lines = []
            
            for i, line in enumerate(lines, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    if '>=' in line or '==' in line or '~=' in line:
                        packages.append(line)
                    elif line and not line.startswith('-'):
                        invalid_lines.append(f"Line {i}: {line}")
            
            self.result.add_test_result(
                test_name,
                'PASSED' if not invalid_lines else 'FAILED',
                f"Found {len(packages)} packages, {len(invalid_lines)} invalid lines",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def run_all(self):
        """ëª¨ë“  ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ“ ì„¤ì • íŒŒì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_methods = [
            self.test_required_files_exist,
            self.test_python_modules_exist,
            self.test_json_files_valid,
            self.test_requirements_parseable
        ]
        
        for test_method in test_methods:
            try:
                test_method()
            except Exception as e:
                logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {test_method.__name__}: {e}")
        
        return self.result


class ModuleIntegrityTests:
    """ëª¨ë“ˆ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.result = SafeTestResult()
    
    def test_python_syntax(self):
        """íŒŒì´ì¬ íŒŒì¼ ë¬¸ë²• ê²€ì‚¬"""
        start_time = time.time()
        test_name = "python_syntax_check"
        
        try:
            python_files = [f for f in os.listdir('.') if f.endswith('.py')]
            
            syntax_errors = []
            valid_files = []
            
            for py_file in python_files:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        code = f.read()
                    
                    # ë¬¸ë²• ê²€ì‚¬
                    compile(code, py_file, 'exec')
                    valid_files.append(py_file)
                    
                except SyntaxError as e:
                    syntax_errors.append(f"{py_file}:{e.lineno}: {e.msg}")
                except Exception as e:
                    syntax_errors.append(f"{py_file}: {str(e)}")
            
            self.result.add_test_result(
                test_name,
                'PASSED' if not syntax_errors else 'FAILED',
                f"Valid: {len(valid_files)}, Errors: {len(syntax_errors)}. First 3: {syntax_errors[:3]}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_import_structure(self):
        """import êµ¬ì¡° ê²€ì‚¬"""
        start_time = time.time()
        test_name = "import_structure_check"
        
        try:
            python_files = [f for f in os.listdir('.') if f.endswith('.py')]
            
            import_issues = []
            clean_files = []
            
            for py_file in python_files:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # import ë¬¸ ê²€ì‚¬
                    for i, line in enumerate(lines, 1):
                        line = line.strip()
                        if line.startswith('from') or line.startswith('import'):
                            # ìƒëŒ€ import ê²€ì‚¬
                            if 'from .' in line and not py_file.startswith('test_'):
                                import_issues.append(f"{py_file}:{i}: Relative import in non-package")
                    
                    if py_file not in [issue.split(':')[0] for issue in import_issues]:
                        clean_files.append(py_file)
                        
                except Exception as e:
                    import_issues.append(f"{py_file}: Read error - {str(e)}")
            
            self.result.add_test_result(
                test_name,
                'PASSED' if len(import_issues) <= 2 else 'FAILED',  # ì•½ê°„ì˜ ì—¬ìœ  í—ˆìš©
                f"Clean: {len(clean_files)}, Issues: {len(import_issues)}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_file_encoding(self):
        """íŒŒì¼ ì¸ì½”ë”© ê²€ì‚¬"""
        start_time = time.time()
        test_name = "file_encoding_check"
        
        try:
            python_files = [f for f in os.listdir('.') if f.endswith('.py')]
            
            encoding_errors = []
            valid_files = []
            
            for py_file in python_files:
                try:
                    # UTF-8ìœ¼ë¡œ ì½ê¸° ì‹œë„
                    with open(py_file, 'r', encoding='utf-8') as f:
                        f.read()
                    valid_files.append(py_file)
                    
                except UnicodeDecodeError as e:
                    encoding_errors.append(f"{py_file}: UTF-8 decode error")
                except Exception as e:
                    encoding_errors.append(f"{py_file}: {str(e)}")
            
            self.result.add_test_result(
                test_name,
                'PASSED' if not encoding_errors else 'FAILED',
                f"UTF-8 compatible: {len(valid_files)}, Errors: {len(encoding_errors)}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def run_all(self):
        """ëª¨ë“  ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ” ëª¨ë“ˆ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_methods = [
            self.test_python_syntax,
            self.test_import_structure,
            self.test_file_encoding
        ]
        
        for test_method in test_methods:
            try:
                test_method()
            except Exception as e:
                logger.error(f"ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {test_method.__name__}: {e}")
        
        return self.result


class PerformanceTests:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.result = SafeTestResult()
    
    def test_file_load_performance(self):
        """íŒŒì¼ ë¡œë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "file_load_performance"
        
        try:
            python_files = [f for f in os.listdir('.') if f.endswith('.py')][:10]  # ìµœëŒ€ 10ê°œ
            
            load_times = []
            
            for py_file in python_files:
                file_start = time.time()
                with open(py_file, 'r', encoding='utf-8') as f:
                    f.read()
                load_time = time.time() - file_start
                load_times.append(load_time)
            
            avg_load_time = sum(load_times) / len(load_times) if load_times else 0
            max_load_time = max(load_times) if load_times else 0
            
            # ì„±ëŠ¥ ê¸°ì¤€: í‰ê·  0.1ì´ˆ ì´í•˜, ìµœëŒ€ 0.5ì´ˆ ì´í•˜
            performance_ok = avg_load_time < 0.1 and max_load_time < 0.5
            
            self.result.add_test_result(
                test_name,
                'PASSED' if performance_ok else 'FAILED',
                f"Avg: {avg_load_time:.3f}s, Max: {max_load_time:.3f}s, Files: {len(python_files)}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def test_json_parsing_performance(self):
        """JSON íŒŒì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "json_parsing_performance"
        
        try:
            json_files = [f for f in os.listdir('.') if f.endswith('.json')]
            
            if not json_files:
                self.result.add_test_result(
                    test_name,
                    'SKIPPED',
                    "No JSON files found",
                    time.time() - start_time
                )
                return
            
            parse_times = []
            
            for json_file in json_files:
                try:
                    file_start = time.time()
                    with open(json_file, 'r', encoding='utf-8') as f:
                        json.load(f)
                    parse_time = time.time() - file_start
                    parse_times.append(parse_time)
                except:
                    continue  # íŒŒì‹± ë¶ˆê°€ëŠ¥í•œ íŒŒì¼ì€ ìŠ¤í‚µ
            
            if not parse_times:
                self.result.add_test_result(
                    test_name,
                    'SKIPPED',
                    "No valid JSON files",
                    time.time() - start_time
                )
                return
            
            avg_parse_time = sum(parse_times) / len(parse_times)
            max_parse_time = max(parse_times)
            
            # ì„±ëŠ¥ ê¸°ì¤€: í‰ê·  0.05ì´ˆ ì´í•˜
            performance_ok = avg_parse_time < 0.05
            
            self.result.add_test_result(
                test_name,
                'PASSED' if performance_ok else 'FAILED',
                f"Avg: {avg_parse_time:.3f}s, Max: {max_parse_time:.3f}s, Files: {len(parse_times)}",
                time.time() - start_time
            )
            
        except Exception as e:
            self.result.add_test_result(
                test_name,
                'FAILED',
                str(e),
                time.time() - start_time
            )
    
    def run_all(self):
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_methods = [
            self.test_file_load_performance,
            self.test_json_parsing_performance
        ]
        
        for test_method in test_methods:
            try:
                test_method()
            except Exception as e:
                logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {test_method.__name__}: {e}")
        
        return self.result


class SafeTestRunner:
    """ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.all_results = {}
        self.start_runtime = time.time()
    
    def run_safe_tests(self, test_types: List[str] = None):
        """ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if test_types is None:
            test_types = ['config', 'integrity', 'performance']
        
        print("="*67)
        print("                  G4K ì•ˆì „ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸                     ")
        print("="*67)
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"í…ŒìŠ¤íŠ¸ íƒ€ì…: {', '.join(test_types)}")
        print("-" * 67)
        
        if 'config' in test_types:
            config_tests = ConfigurationTests()
            self.all_results['configuration'] = config_tests.run_all()
        
        if 'integrity' in test_types:
            integrity_tests = ModuleIntegrityTests()
            self.all_results['integrity'] = integrity_tests.run_all()
        
        if 'performance' in test_types:
            performance_tests = PerformanceTests()
            self.all_results['performance'] = performance_tests.run_all()
        
        # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self._generate_comprehensive_report()
        
        return self.all_results
    
    def _generate_comprehensive_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        total_runtime = time.time() - self.start_runtime
        
        print("\n" + "="*67)
        print("ğŸ“Š ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*67)
        
        overall_stats = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'skipped': 0
        }
        
        # ê° í…ŒìŠ¤íŠ¸ íƒ€ì…ë³„ ê²°ê³¼
        for test_type, result in self.all_results.items():
            summary = result.get_summary()
            overall_stats['total'] += summary['total']
            overall_stats['passed'] += summary['passed']
            overall_stats['failed'] += summary['failed']
            overall_stats['skipped'] += summary['skipped']
            
            status_icon = "âœ…" if summary['failed'] == 0 else "âŒ"
            print(f"{status_icon} {test_type.upper():12} - {summary['passed']:2d}/{summary['total']:2d} passed ({summary['success_rate']:5.1f}%)")
        
        # ì „ì²´ í†µê³„
        overall_success_rate = (overall_stats['passed'] / overall_stats['total'] * 100) if overall_stats['total'] > 0 else 0
        
        print("-" * 67)
        print(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸:     {overall_stats['total']:3d}")
        print(f"   âœ… ì„±ê³µ:       {overall_stats['passed']:3d} ({overall_success_rate:.1f}%)")
        print(f"   âŒ ì‹¤íŒ¨:       {overall_stats['failed']:3d}")
        print(f"   â­ï¸ ìŠ¤í‚µ:       {overall_stats['skipped']:3d}")
        print(f"   â±ï¸ ì‹¤í–‰ì‹œê°„:   {total_runtime:.2f}ì´ˆ")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê²°ê³¼ ë¶„ì„:")
        if overall_success_rate >= 95:
            print("   ğŸ‰ ì™„ë²½! ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ê±°ì˜ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        elif overall_success_rate >= 80:
            print("   ğŸ‘ ì–‘í˜¸! ëŒ€ë¶€ë¶„ì˜ í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        elif overall_success_rate >= 60:
            print("   âš ï¸  ì£¼ì˜! ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            print("   ğŸš¨ ê²½ê³ ! ë§ì€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì˜¤ë¥˜ ìƒì„¸ (ì²˜ìŒ 5ê°œë§Œ)
        all_errors = []
        for result in self.all_results.values():
            all_errors.extend(result.get_summary()['errors'])
        
        if all_errors:
            print(f"\nâŒ ì£¼ìš” ì˜¤ë¥˜ (ì´ {len(all_errors)}ê°œ):")
            for i, error in enumerate(all_errors[:5], 1):
                print(f"   {i}. {error}")
            if len(all_errors) > 5:
                print(f"   ... ë° {len(all_errors) - 5}ê°œ ì¶”ê°€ ì˜¤ë¥˜")
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        self._save_json_report(overall_stats, total_runtime)
    
    def _save_json_report(self, overall_stats, total_runtime):
        """JSON í˜•íƒœë¡œ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"safe_test_report_{timestamp}.json"
        
        report_data = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'total_runtime': total_runtime,
                'test_types': list(self.all_results.keys())
            },
            'overall_stats': overall_stats,
            'detailed_results': {}
        }
        
        for test_type, result in self.all_results.items():
            report_data['detailed_results'][test_type] = {
                'summary': result.get_summary(),
                'individual_tests': result.results
            }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        except Exception as e:
            print(f"âš ï¸ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='G4K ì•ˆì „ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸')
    parser.add_argument('--type', 
                       choices=['config', 'integrity', 'performance', 'all'],
                       default='all',
                       help='ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ íƒ€ì…')
    parser.add_argument('--quick', 
                       action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (í•µì‹¬ í•­ëª©ë§Œ)')
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ íƒ€ì… ê²°ì •
    if args.type == 'all':
        test_types = ['config', 'integrity', 'performance']
    else:
        test_types = [args.type]
    
    if args.quick:
        test_types = ['config']  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ëŠ” ì„¤ì •ë§Œ
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = SafeTestRunner()
    runner.run_safe_tests(test_types)


if __name__ == "__main__":
    main()